# Video Assembly Monitor: Dataset & Model Pipeline

Welcome to the data and model development module of the **Video Assembly Monitor** project!  
This section focuses on enabling accurate **hand tracking** and **object detection**, forming the foundational layer of assembly stage recognition and real-time verification.

This work is part of the larger system **Hand Tracking and Object Detection for Assembly Stage Verification and Improvement**, jointly developed with backend, frontend, and vector analysis modules.

---

## ðŸ“¦ Project Overview

This module is responsible for:

- Capturing and annotating multi-perspective assembly videos
- Defining and segmenting object classes and action sequences
- Training and evaluating object detection models (YOLOv11/YOLOv12)
- Providing hand-annotated ground-truth for gesture and action recognition

---

## ðŸŽ¥ Dataset Construction

### 1. ðŸ“¹ Video Recording

Two camera views were used to simulate realistic assembly processes:

- **Main View**: Frontal angle capturing full-hand motion
- **Side View**: Profile perspective ensuring spatial validation

> All recordings follow standard environment setup: consistent indoor lighting, neutral background, standard uniform.

### 2. âœï¸ Annotation Process

#### ðŸŸ  Roboflow

- Extract key frames from video for manual annotation
- Define object bounding boxes and labels
- Use auto-labeling to expand and validate dataset
- Export in YOLO format

#### ðŸ”µ VIA (VGG Image Annotator)

- Annotate video keyframes for gesture segmentation
- Define action intervals and batch-label steps
- Export JSON annotations for vector machine use

---

## ðŸ§© Assembly Semantics Definition

### 1. ðŸ“¦ Object & Action Classes

Defined components include hands, base, screws, lid, tools, etc.

### 2. ðŸ” Assembly Sequence & Preset

Each assembly task is pre-defined in a **Preset Assembly Plan**. These are used in:

- Vector sequence validation
- Gesture classification
- Task step matching

### 3. â±ï¸ Standard Time Definition

Standard operating times (SOT) are defined for each step to:

- Monitor delay or timeout
- Enable feedback for performance tuning

ðŸ“„ [Click here to view the full demo document (assembly sequence, definitions, timing)](your-demo-link-here)

---

## âš™ï¸ Roboflow Configuration Guide

### 1. ðŸ§¹ Preprocessing

- **Auto-Orient**: Remove EXIF rotation artifacts
- **Resize**: Normalize image dimensions

### 2. ðŸ”„ Data Augmentation

| Technique        | Description                                      |
|------------------|--------------------------------------------------|
| Flip             | Random horizontal/vertical flip                  |
| Rotation         | Â±15Â° variation for viewpoint robustness          |
| Grayscale        | Applied to 15% of training images                |
| Hue Adjustment   | Random Â±18Â° color shift                          |
| Gaussian Blur    | Up to 2.6px for motion/focus noise resistance    |

---

## ðŸ§  Model Training

### YOLOv11 / YOLOv12

- **Pre-trained** on MS COCO v28 (47.0% mAP)
- **Transfer Learning** from Roboflow Universe
- **Backbone**: Efficient, real-time object detection

---

## ðŸ“Š Model Evaluation

### 1. Confusion Matrix

- Diagonal: Correct class predictions  
- Off-diagonal: Misclassification indicators  
- Use for quality assessment and mislabeling detection

### 2. Vector Analysis

- Project F1 scores of samples into 2D semantic space  
- Reveal clusters and anomalies in action recognition  
- Helps align with vector-based sequence validation

---

## ðŸ”„ EOID Integration & Deployment

This module integrates into the EOID (Event-Oriented Intelligent Detection) engine.

### ðŸ“‚ Deployment Steps

- Adapt data format for EOID  
- Migrate and map dataset fields  
- Ensure evaluation logic and result exports align with backend logic

### ðŸ§¾ Key Files

- `transfer.py`: Handles Roboflow-to-EOID conversion  
- `app.py`: Client for interacting with EOID API endpoints  

---

## ðŸ¤– YOLO Inference Sample

To test model predictions locally:

```bash
python yolov5/detect.py --weights path/to/best.pt --img 640 --conf 0.4 --source your_image_or_video.mp4
>>>>>>> 20a7ade521077e019b83fba7f9826cc36c85c885
